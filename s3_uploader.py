#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
S3 íŒŒí‹°ì…”ë‹ ì—…ë¡œë“œ ê´€ë¦¬ì

ê¸°ëŠ¥:
1. ìƒì„±ëœ CSV íŒŒì¼ì„ S3ì— íŒŒí‹°ì…˜ êµ¬ì¡°ë¡œ ì—…ë¡œë“œ
2. year={year}/mm={month} íŒŒí‹°ì…˜ êµ¬ì¡° ìƒì„±
3. íŒŒì¼ëª…ì—ì„œ ë…„ë„/ì›” ì •ë³´ ì¶”ì¶œí•˜ì—¬ ìë™ íŒŒí‹°ì…”ë‹
4. ì—…ë¡œë“œ ì§„í–‰ìƒí™© ì¶”ì  ë° ì˜¤ë¥˜ ê´€ë¦¬

íŒŒí‹°ì…˜ êµ¬ì¡°:
s3://bucket/prefix/year=2025/mm=06/corp_code=00171636/report_type=BS/FS_00171636_202506.parquet
"""

import os
import boto3
from pathlib import Path
import pandas as pd
from datetime import datetime
import re
from typing import List, Dict, Optional
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


class S3Uploader:
    """S3 íŒŒí‹°ì…”ë‹ ì—…ë¡œë“œ ê´€ë¦¬ì"""

    def __init__(self, dry_run=False):
        """
        ì´ˆê¸°í™”

        Args:
            dry_run (bool): Trueì´ë©´ ì‹¤ì œ ì—…ë¡œë“œ ì—†ì´ ì‹œë®¬ë ˆì´ì…˜ë§Œ ìˆ˜í–‰
        """
        self.dry_run = dry_run
        # S3 ì„¤ì • ë¡œë“œ
        self.bucket_name = os.getenv('S3_BUCKET_NAME')
        self.s3_prefix = os.getenv('S3_PREFIX', '').rstrip('/')

        if not self.bucket_name:
            raise ValueError("S3_BUCKET_NAMEì´ .env íŒŒì¼ì— ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

        # S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        if self.dry_run:
            print(f"[DRY-RUN MODE] S3 í´ë¼ì´ì–¸íŠ¸ ì‹œë®¬ë ˆì´ì…˜")
            print(f"  - ë²„í‚·: {self.bucket_name}")
            print(f"  - í”„ë¦¬í”½ìŠ¤: {self.s3_prefix}")
            self.s3_client = None
        else:
            try:
                self.s3_client = boto3.client('s3')
                print(f"S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
                print(f"  - ë²„í‚·: {self.bucket_name}")
                print(f"  - í”„ë¦¬í”½ìŠ¤: {self.s3_prefix}")
            except Exception as e:
                print(f"S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.s3_client = None

        # ì—…ë¡œë“œ í†µê³„
        self.stats = {
            "files_uploaded": 0,
            "files_failed": 0,
            "total_size": 0,
            "errors": []
        }

    def extract_partition_info(self, filename: str, parquet_data: Optional[pd.DataFrame] = None) -> Optional[Dict[str, str]]:
        """
        íŒŒì¼ëª…ê³¼ ë°ì´í„°ì—ì„œ íŒŒí‹°ì…˜ ì •ë³´ ì¶”ì¶œ

        Args:
            filename (str): íŒŒì¼ëª… (ì˜ˆ: "FS_00171636_202506.parquet")
            parquet_data (pd.DataFrame): Parquet ë°ì´í„° (corp_code, report_type ì¶”ì¶œìš©)

        Returns:
            dict: {"year": "2025", "month": "06", "corp_code": "00171636", "report_type": "BS"} ë˜ëŠ” None
        """
        # FS_íšŒì‚¬ì½”ë“œ_YYYYMM.parquet íŒ¨í„´ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        pattern = r'FS_(\d{8})_(\d{4})(\d{2})\.parquet'
        match = re.search(pattern, filename)

        if not match:
            print(f"íŒŒì¼ëª… íŒ¨í„´ì´ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤: {filename}")
            return None

        corp_code = match.group(1)
        year = match.group(2)
        month = match.group(3)

        # ê¸°ë³¸ íŒŒí‹°ì…˜ ì •ë³´
        partition_info = {
            "year": year,
            "month": month,
            "corp_code": corp_code
        }

        # Parquet ë°ì´í„°ì—ì„œ report_type ì¶”ì¶œ (ìš°ì„ ìˆœìœ„: BS > CIS)
        if parquet_data is not None and not parquet_data.empty:
            if 'report_type' in parquet_data.columns:
                unique_report_types = parquet_data['report_type'].unique()
                # BS (ì¬ë¬´ìƒíƒœí‘œ)ê°€ ìˆìœ¼ë©´ ìš°ì„ , ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ íƒ€ì… ì‚¬ìš©
                if 'BS' in unique_report_types:
                    partition_info['report_type'] = 'BS'
                elif len(unique_report_types) > 0:
                    partition_info['report_type'] = str(unique_report_types[0])
                else:
                    partition_info['report_type'] = 'UNKNOWN'
            else:
                print(f"ë°ì´í„°ì— report_type ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {filename}")
                partition_info['report_type'] = 'UNKNOWN'
        else:
            # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ íŒŒì¼ëª…ìœ¼ë¡œ ì¶”ì •
            partition_info['report_type'] = 'MIXED'

        return partition_info

    def generate_s3_key(self, filename: str, partition_info: Dict[str, str]) -> str:
        """
        S3 í‚¤ ìƒì„± (í™•ì¥ëœ íŒŒí‹°ì…˜ ê²½ë¡œ í¬í•¨)

        Args:
            filename (str): íŒŒì¼ëª…
            partition_info (dict): íŒŒí‹°ì…˜ ì •ë³´ {"year": "2025", "month": "06", "corp_code": "00171636", "report_type": "BS"}

        Returns:
            str: S3 í‚¤ ê²½ë¡œ
        """
        year = partition_info.get('year', 'unknown')
        month = partition_info.get('month', 'unknown')
        corp_code = partition_info.get('corp_code', 'unknown')
        report_type = partition_info.get('report_type', 'unknown')

        # íŒŒí‹°ì…˜ ê²½ë¡œ ìƒì„±: year=YYYY/mm=MM/corp_code=XXXXXXXX/report_type=XX/filename
        partition_path = f"year={year}/mm={month}/corp_code={corp_code}/report_type={report_type}"

        if self.s3_prefix:
            s3_key = f"{self.s3_prefix}/{partition_path}/{filename}"
        else:
            s3_key = f"{partition_path}/{filename}"

        return s3_key

    def prepare_parquet_for_upload(self, parquet_file_path: str) -> Optional[Dict[str, any]]:
        """
        =========================================================================
        ğŸ—‚ï¸ ì¤‘ìš”: Parquet íŒŒì¼ì—ì„œ íŒŒí‹°ì…˜ ì»¬ëŸ¼ ì œê±° ğŸ—‚ï¸
        =========================================================================

        ëª©ì : S3 íŒŒí‹°ì…˜ êµ¬ì¡°ë¡œ ì €ì¥ ì‹œ yyyy, month ì»¬ëŸ¼ì€ ë¶ˆí•„ìš”

        ì œê±° ëŒ€ìƒ:
        - yyyy ì»¬ëŸ¼: íŒŒí‹°ì…˜ ê²½ë¡œì˜ year=YYYYë¡œ ëŒ€ì²´
        - month ì»¬ëŸ¼: íŒŒí‹°ì…˜ ê²½ë¡œì˜ mm=MMìœ¼ë¡œ ëŒ€ì²´

        ìœ ì§€ ëŒ€ìƒ:
        - ë‚˜ë¨¸ì§€ ëª¨ë“  ì»¬ëŸ¼ (order_no, corp_name, concept_id, label_ko, value ë“±)
        - corp_code, report_typeì€ íŒŒí‹°ì…˜ìœ¼ë¡œ í™œìš©ë˜ë¯€ë¡œ íŒŒì¼ì—ì„œëŠ” ì œê±° ê°€ëŠ¥

        ìˆ˜ì •ë°©ë²•:
        - ë‹¤ì‹œ yyyy, monthë¥¼ í¬í•¨í•˜ë ¤ë©´ drop_columns ë¦¬ìŠ¤íŠ¸ì—ì„œ ì œê±°
        - corp_code, report_typeì„ íŒŒì¼ì— ìœ ì§€í•˜ë ¤ë©´ drop_columnsì—ì„œ ì œê±°
        =========================================================================

        Args:
            parquet_file_path (str): ì›ë³¸ Parquet íŒŒì¼ ê²½ë¡œ

        Returns:
            dict: {"temp_file_path": str, "original_data": pd.DataFrame} ë˜ëŠ” None
        """
        try:
            # Parquet íŒŒì¼ ì½ê¸°
            df = pd.read_parquet(parquet_file_path)

            # íŒŒí‹°ì…˜ ì»¬ëŸ¼ ì œê±° (yyyy, month, corp_code, report_type)
            # QuickSightì—ì„œ íŒŒí‹°ì…˜ìœ¼ë¡œ í•„í„°ë§í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë°ì´í„°ì—ì„œëŠ” ì œê±°
            drop_columns = ['yyyy', 'month', 'corp_code', 'report_type']
            columns_to_drop = [col for col in drop_columns if col in df.columns]

            if columns_to_drop:
                df_cleaned = df.drop(columns=columns_to_drop)
                print(f"íŒŒí‹°ì…˜ ì»¬ëŸ¼ ì œê±°: {columns_to_drop}")
                print(f"  - ì œê±° ì „: {len(df.columns)}ê°œ ì»¬ëŸ¼")
                print(f"  - ì œê±° í›„: {len(df_cleaned.columns)}ê°œ ì»¬ëŸ¼")
            else:
                df_cleaned = df
                print("ì œê±°í•  íŒŒí‹°ì…˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            temp_file_path = parquet_file_path.replace('.parquet', '_temp_for_s3.parquet')
            df_cleaned.to_parquet(temp_file_path, index=False)

            return {
                "temp_file_path": temp_file_path,
                "original_data": df  # íŒŒí‹°ì…˜ ì •ë³´ ì¶”ì¶œìš©
            }

        except Exception as e:
            print(f"Parquet íŒŒì¼ ì „ì²˜ë¦¬ ì˜¤ë¥˜ ({parquet_file_path}): {e}")
            return None

    def upload_file_to_s3(self, local_file_path: str, s3_key: str) -> bool:
        """
        íŒŒì¼ì„ S3ì— ì—…ë¡œë“œ

        Args:
            local_file_path (str): ë¡œì»¬ íŒŒì¼ ê²½ë¡œ
            s3_key (str): S3 í‚¤ ê²½ë¡œ

        Returns:
            bool: ì—…ë¡œë“œ ì„±ê³µ ì—¬ë¶€
        """
        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size = os.path.getsize(local_file_path)

        if self.dry_run:
            print(f"[DRY-RUN] S3 ì—…ë¡œë“œ ì‹œë®¬ë ˆì´ì…˜: s3://{self.bucket_name}/{s3_key}")
            print(f"  - íŒŒì¼ í¬ê¸°: {file_size:,} bytes")

            # í†µê³„ ì—…ë°ì´íŠ¸ (ì‹œë®¬ë ˆì´ì…˜)
            self.stats["files_uploaded"] += 1
            self.stats["total_size"] += file_size
            return True

        if not self.s3_client:
            print("S3 í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False

        try:
            # S3 ì—…ë¡œë“œ
            self.s3_client.upload_file(
                local_file_path,
                self.bucket_name,
                s3_key
            )

            print(f"S3 ì—…ë¡œë“œ ì„±ê³µ: s3://{self.bucket_name}/{s3_key}")
            print(f"  - íŒŒì¼ í¬ê¸°: {file_size:,} bytes")

            # í†µê³„ ì—…ë°ì´íŠ¸
            self.stats["files_uploaded"] += 1
            self.stats["total_size"] += file_size

            return True

        except Exception as e:
            error_msg = f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨ ({s3_key}): {e}"
            print(error_msg)
            self.stats["errors"].append(error_msg)
            self.stats["files_failed"] += 1
            return False

    def filter_and_upload_by_partitions(self, parquet_files: List[str]) -> Dict:
        """
        Parquet íŒŒì¼ë“¤ì„ íŒŒí‹°ì…˜ë³„ë¡œ í•„í„°ë§í•˜ì—¬ S3ì— ì—…ë¡œë“œ

        ê° íŒŒì¼ì„ corp_code ë° report_typeë³„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì—…ë¡œë“œí•©ë‹ˆë‹¤.
        ë™ì¼í•œ corp_code/report_type ì¡°í•©ì´ ì—¬ëŸ¬ íŒŒì¼ì— ìˆì„ ê²½ìš°,
        ë³„ë„ì˜ íŒŒì¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            parquet_files (list): Parquet íŒŒì¼ ê²½ë¡œ ëª©ë¡

        Returns:
            dict: ì—…ë¡œë“œ ê²°ê³¼ í†µê³„
        """
        print(f"\n=== S3 íŒŒí‹°ì…˜ë³„ í•„í„°ë§ ì—…ë¡œë“œ ì‹œì‘ ===")
        print(f"ì—…ë¡œë“œí•  íŒŒì¼ ìˆ˜: {len(parquet_files)}")
        if self.dry_run:
            print(f"[DRY-RUN MODE] ì‹¤ì œ ì—…ë¡œë“œ ì—†ì´ ì‹œë®¬ë ˆì´ì…˜ë§Œ ìˆ˜í–‰")

        if not self.dry_run and not self.s3_client:
            print("S3 í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return self.stats

        uploaded_files = []
        temp_files_to_cleanup = []
        partition_file_groups = {}  # corp_code + report_typeë³„ë¡œ ê·¸ë£¹í™”

        # 1ë‹¨ê³„: ëª¨ë“  íŒŒì¼ì˜ ë°ì´í„°ë¥¼ íŒŒí‹°ì…˜ë³„ë¡œ ê·¸ë£¹í™”
        for i, parquet_file in enumerate(parquet_files, 1):
            print(f"\n[{i}/{len(parquet_files)}] ë¶„ì„ ì¤‘: {Path(parquet_file).name}")

            try:
                # ì›ë³¸ ë°ì´í„° ë¡œë“œ
                df = pd.read_parquet(parquet_file)

                if df.empty:
                    print(f"  ë¹ˆ íŒŒì¼, ê±´ë„ˆëœ€")
                    continue

                # íŒŒí‹°ì…˜ ì •ë³´ ì¶”ì¶œ
                filename = Path(parquet_file).name
                partition_info = self.extract_partition_info(filename, df)

                if not partition_info:
                    print(f"  íŒŒí‹°ì…˜ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨, ê±´ë„ˆëœ€")
                    continue

                year = partition_info["year"]
                month = partition_info["month"]
                base_corp_code = partition_info["corp_code"]

                # corp_code ë° report_typeë³„ë¡œ ë°ì´í„° ë¶„ë¦¬
                unique_partitions = []

                if 'corp_code' in df.columns and 'report_type' in df.columns:
                    # ì‹¤ì œ ë°ì´í„°ì˜ corp_codeì™€ report_type ì¡°í•© í™•ì¸
                    partition_combinations = df[['corp_code', 'report_type']].drop_duplicates()

                    for _, row in partition_combinations.iterrows():
                        corp_code = str(row['corp_code']).zfill(8)
                        report_type = str(row['report_type'])

                        # í•´ë‹¹ íŒŒí‹°ì…˜ì˜ ë°ì´í„°ë§Œ í•„í„°ë§
                        partition_data = df[(df['corp_code'] == row['corp_code']) &
                                          (df['report_type'] == row['report_type'])].copy()

                        if not partition_data.empty:
                            partition_key = f"{year}_{month}_{corp_code}_{report_type}"

                            if partition_key not in partition_file_groups:
                                partition_file_groups[partition_key] = {
                                    'year': year,
                                    'month': month,
                                    'corp_code': corp_code,
                                    'report_type': report_type,
                                    'data_frames': [],
                                    'source_files': []
                                }

                            partition_file_groups[partition_key]['data_frames'].append(partition_data)
                            partition_file_groups[partition_key]['source_files'].append(parquet_file)

                            print(f"  íŒŒí‹°ì…˜ {partition_key}: {len(partition_data)}ê°œ í–‰")

                else:
                    # corp_code, report_type ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° íŒŒì¼ëª… ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬
                    report_type = partition_info.get('report_type', 'MIXED')
                    partition_key = f"{year}_{month}_{base_corp_code}_{report_type}"

                    if partition_key not in partition_file_groups:
                        partition_file_groups[partition_key] = {
                            'year': year,
                            'month': month,
                            'corp_code': base_corp_code,
                            'report_type': report_type,
                            'data_frames': [],
                            'source_files': []
                        }

                    partition_file_groups[partition_key]['data_frames'].append(df)
                    partition_file_groups[partition_key]['source_files'].append(parquet_file)

                    print(f"  íŒŒí‹°ì…˜ {partition_key}: {len(df)}ê°œ í–‰")

            except Exception as e:
                print(f"  íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue

        # 2ë‹¨ê³„: íŒŒí‹°ì…˜ë³„ë¡œ ë°ì´í„° ë³‘í•© ë° ì—…ë¡œë“œ
        print(f"\n=== ì´ {len(partition_file_groups)}ê°œ íŒŒí‹°ì…˜ ì—…ë¡œë“œ ì‹œì‘ ===")

        for partition_key, partition_data in partition_file_groups.items():
            year = partition_data['year']
            month = partition_data['month']
            corp_code = partition_data['corp_code']
            report_type = partition_data['report_type']

            print(f"\níŒŒí‹°ì…˜ ì²˜ë¦¬: {partition_key}")
            print(f"  ì†ŒìŠ¤ íŒŒì¼: {len(partition_data['source_files'])}ê°œ")

            try:
                # ë°ì´í„° ë³‘í•©
                if len(partition_data['data_frames']) == 1:
                    merged_df = partition_data['data_frames'][0]
                else:
                    merged_df = pd.concat(partition_data['data_frames'], ignore_index=True)

                print(f"  ë³‘í•©ëœ ë°ì´í„°: {len(merged_df)}ê°œ í–‰, {len(merged_df.columns)}ê°œ ì»¬ëŸ¼")

                # íŒŒí‹°ì…˜ ì»¬ëŸ¼ ì œê±°
                drop_columns = ['yyyy', 'month', 'corp_code', 'report_type']
                columns_to_drop = [col for col in drop_columns if col in merged_df.columns]

                if columns_to_drop:
                    merged_df_cleaned = merged_df.drop(columns=columns_to_drop)
                    print(f"  íŒŒí‹°ì…˜ ì»¬ëŸ¼ ì œê±°: {columns_to_drop}")
                else:
                    merged_df_cleaned = merged_df

                # ì„ì‹œ íŒŒì¼ ìƒì„±
                temp_filename = f"FS_{corp_code}_{year}{month}_{report_type}_partitioned.parquet"
                temp_file_path = os.path.join(os.path.dirname(partition_data['source_files'][0]), temp_filename)

                merged_df_cleaned.to_parquet(temp_file_path, index=False)
                temp_files_to_cleanup.append(temp_file_path)

                # S3 í‚¤ ìƒì„±
                partition_info_dict = {
                    'year': year,
                    'month': month,
                    'corp_code': corp_code,
                    'report_type': report_type
                }

                s3_key = self.generate_s3_key(temp_filename, partition_info_dict)
                print(f"  S3 ê²½ë¡œ: s3://{self.bucket_name}/{s3_key}")

                # S3 ì—…ë¡œë“œ
                if self.upload_file_to_s3(temp_file_path, s3_key):
                    uploaded_files.append({
                        "local_files": partition_data['source_files'],
                        "s3_key": s3_key,
                        "partition": f"year={year}/mm={month}/corp_code={corp_code}/report_type={report_type}",
                        "rows_count": len(merged_df)
                    })
                    print(f"  âœ“ ì—…ë¡œë“œ ì„±ê³µ")
                else:
                    print(f"  âœ— ì—…ë¡œë“œ ì‹¤íŒ¨")

            except Exception as e:
                print(f"  íŒŒí‹°ì…˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue

        # 3ë‹¨ê³„: ì„ì‹œ íŒŒì¼ ì •ë¦¬
        self.cleanup_temp_files(temp_files_to_cleanup)

        # 4ë‹¨ê³„: ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±
        self.generate_partition_upload_report(uploaded_files)

        return self.stats

    def upload_parquet_files(self, parquet_files: List[str]) -> Dict:
        """
        ì—¬ëŸ¬ Parquet íŒŒì¼ì„ S3ì— íŒŒí‹°ì…”ë‹í•˜ì—¬ ì—…ë¡œë“œ (ê¸°ì¡´ ë°©ì‹)

        Args:
            parquet_files (list): Parquet íŒŒì¼ ê²½ë¡œ ëª©ë¡

        Returns:
            dict: ì—…ë¡œë“œ ê²°ê³¼ í†µê³„
        """
        print(f"\n=== S3 ê¸°ë³¸ íŒŒí‹°ì…”ë‹ ì—…ë¡œë“œ ì‹œì‘ ===")
        print(f"ì—…ë¡œë“œí•  íŒŒì¼ ìˆ˜: {len(parquet_files)}")
        if self.dry_run:
            print(f"[DRY-RUN MODE] ì‹¤ì œ ì—…ë¡œë“œ ì—†ì´ ì‹œë®¬ë ˆì´ì…˜ë§Œ ìˆ˜í–‰")

        if not self.dry_run and not self.s3_client:
            print("S3 í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return self.stats

        uploaded_files = []
        temp_files_to_cleanup = []

        for i, parquet_file in enumerate(parquet_files, 1):
            print(f"\n[{i}/{len(parquet_files)}] ì²˜ë¦¬ ì¤‘: {Path(parquet_file).name}")

            # 1. Parquet íŒŒì¼ ì „ì²˜ë¦¬ (ì›ë³¸ ë°ì´í„° ë¡œë“œ ë° íŒŒí‹°ì…˜ ì»¬ëŸ¼ ì œê±°)
            filename = Path(parquet_file).name
            prepare_result = self.prepare_parquet_for_upload(parquet_file)

            if not prepare_result:
                print(f"Parquet ì „ì²˜ë¦¬ ì‹¤íŒ¨, ê±´ë„ˆëœ€: {filename}")
                continue

            temp_parquet_path = prepare_result["temp_file_path"]
            original_data = prepare_result["original_data"]
            temp_files_to_cleanup.append(temp_parquet_path)

            # 2. íŒŒí‹°ì…˜ ì •ë³´ ì¶”ì¶œ (ë°ì´í„° í¬í•¨)
            partition_info = self.extract_partition_info(filename, original_data)
            if not partition_info:
                print(f"íŒŒí‹°ì…˜ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨, ê±´ë„ˆëœ€: {filename}")
                continue

            year = partition_info["year"]
            month = partition_info["month"]
            corp_code = partition_info["corp_code"]
            report_type = partition_info["report_type"]

            print(f"  íŒŒí‹°ì…˜: year={year}/mm={month}/corp_code={corp_code}/report_type={report_type}")

            # 3. S3 í‚¤ ìƒì„±
            s3_key = self.generate_s3_key(filename, partition_info)
            print(f"  S3 ê²½ë¡œ: s3://{self.bucket_name}/{s3_key}")

            # 4. S3 ì—…ë¡œë“œ
            if self.upload_file_to_s3(temp_parquet_path, s3_key):
                uploaded_files.append({
                    "local_file": parquet_file,
                    "s3_key": s3_key,
                    "partition": f"year={year}/mm={month}/corp_code={corp_code}/report_type={report_type}"
                })

        # 5. ì„ì‹œ íŒŒì¼ ì •ë¦¬
        self.cleanup_temp_files(temp_files_to_cleanup)

        # 6. ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±
        self.generate_upload_report(uploaded_files)

        return self.stats

    def cleanup_temp_files(self, temp_files: List[str]):
        """ì„ì‹œ íŒŒì¼ ì •ë¦¬"""
        for temp_file in temp_files:
            try:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
                    print(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ: {Path(temp_file).name}")
            except Exception as e:
                print(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ ({temp_file}): {e}")

    def generate_upload_report(self, uploaded_files: List[Dict]):
        """ì—…ë¡œë“œ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±"""
        print(f"\n=== S3 ì—…ë¡œë“œ ê²°ê³¼ ë³´ê³ ì„œ ===")
        print(f"ì—…ë¡œë“œ ì„±ê³µ: {self.stats['files_uploaded']}ê°œ")
        print(f"ì—…ë¡œë“œ ì‹¤íŒ¨: {self.stats['files_failed']}ê°œ")
        print(f"ì´ ì—…ë¡œë“œ í¬ê¸°: {self.stats['total_size']:,} bytes")

        if uploaded_files:
            print(f"\nì—…ë¡œë“œëœ íŒŒì¼ ëª©ë¡:")
            for file_info in uploaded_files:
                print(f"  - {Path(file_info['local_file']).name}")
                print(f"    â””â”€ s3://{self.bucket_name}/{file_info['s3_key']}")

        if self.stats["errors"]:
            print(f"\në°œìƒí•œ ì˜¤ë¥˜ ({len(self.stats['errors'])}ê°œ):")
            for error in self.stats["errors"][:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                print(f"  - {error}")
            if len(self.stats["errors"]) > 5:
                print(f"  ... ì´ {len(self.stats['errors'])}ê°œ ì˜¤ë¥˜")

    def test_s3_connection(self) -> bool:
        """S3 ì—°ê²° í…ŒìŠ¤íŠ¸"""
        if not self.s3_client:
            return False

        try:
            # ë²„í‚· ì¡´ì¬ í™•ì¸
            self.s3_client.head_bucket(Bucket=self.bucket_name)
            print(f"S3 ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ: {self.bucket_name}")
            return True
        except Exception as e:
            print(f"S3 ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return False


def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    uploader = S3Uploader()

    # S3 ì—°ê²° í…ŒìŠ¤íŠ¸
    if uploader.test_s3_connection():
        print("S3 ì—°ê²°ì´ ì •ìƒì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("S3 ì—°ê²° ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    main()