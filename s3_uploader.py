#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
S3 íŒŒí‹°ì…”ë‹ ì—…ë¡œë“œ ê´€ë¦¬ì

ê¸°ëŠ¥:
1. ìƒì„±ëœ CSV íŒŒì¼ì„ S3ì— íŒŒí‹°ì…˜ êµ¬ì¡°ë¡œ ì—…ë¡œë“œ
2. year={year}/mm={month} íŒŒí‹°ì…˜ êµ¬ì¡° ìƒì„±
3. íŒŒì¼ëª…ì—ì„œ ë…„ë„/ì›” ì •ë³´ ì¶”ì¶œí•˜ì—¬ ìë™ íŒŒí‹°ì…”ë‹
4. ì—…ë¡œë“œ ì§„í–‰ìƒí™© ì¶”ì  ë° ì˜¤ë¥˜ ê´€ë¦¬

íŒŒí‹°ì…˜ êµ¬ì¡°:
s3://bucket/prefix/year=2025/mm=06/FS_íšŒì‚¬ì½”ë“œ_202506.csv
"""

import os
import boto3
from pathlib import Path
import pandas as pd
from datetime import datetime
import re
from typing import List, Dict, Optional
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


class S3Uploader:
    """S3 íŒŒí‹°ì…”ë‹ ì—…ë¡œë“œ ê´€ë¦¬ì"""

    def __init__(self, dry_run=False):
        """
        ì´ˆê¸°í™”

        Args:
            dry_run (bool): Trueì´ë©´ ì‹¤ì œ ì—…ë¡œë“œ ì—†ì´ ì‹œë®¬ë ˆì´ì…˜ë§Œ ìˆ˜í–‰
        """
        self.dry_run = dry_run
        # S3 ì„¤ì • ë¡œë“œ
        self.bucket_name = os.getenv('S3_BUCKET_NAME')
        self.s3_prefix = os.getenv('S3_PREFIX', '').rstrip('/')

        if not self.bucket_name:
            raise ValueError("S3_BUCKET_NAMEì´ .env íŒŒì¼ì— ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

        # S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        if self.dry_run:
            print(f"[DRY-RUN MODE] S3 í´ë¼ì´ì–¸íŠ¸ ì‹œë®¬ë ˆì´ì…˜")
            print(f"  - ë²„í‚·: {self.bucket_name}")
            print(f"  - í”„ë¦¬í”½ìŠ¤: {self.s3_prefix}")
            self.s3_client = None
        else:
            try:
                self.s3_client = boto3.client('s3')
                print(f"S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
                print(f"  - ë²„í‚·: {self.bucket_name}")
                print(f"  - í”„ë¦¬í”½ìŠ¤: {self.s3_prefix}")
            except Exception as e:
                print(f"S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.s3_client = None

        # ì—…ë¡œë“œ í†µê³„
        self.stats = {
            "files_uploaded": 0,
            "files_failed": 0,
            "total_size": 0,
            "errors": []
        }

    def extract_partition_info(self, filename: str) -> Optional[Dict[str, str]]:
        """
        íŒŒì¼ëª…ì—ì„œ íŒŒí‹°ì…˜ ì •ë³´ ì¶”ì¶œ

        Args:
            filename (str): íŒŒì¼ëª… (ì˜ˆ: "FS_00171636_202506.csv")

        Returns:
            dict: {"year": "2025", "month": "06"} ë˜ëŠ” None
        """
        # FS_íšŒì‚¬ì½”ë“œ_YYYYMM.parquet íŒ¨í„´ì—ì„œ YYYYMM ì¶”ì¶œ
        pattern = r'FS_\d{8}_(\d{4})(\d{2})\.parquet'
        match = re.search(pattern, filename)

        if match:
            year = match.group(1)
            month = match.group(2)
            return {
                "year": year,
                "month": month
            }

        print(f"íŒŒì¼ëª…ì—ì„œ íŒŒí‹°ì…˜ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}")
        return None

    def generate_s3_key(self, filename: str, year: str, month: str) -> str:
        """
        S3 í‚¤ ìƒì„± (íŒŒí‹°ì…˜ ê²½ë¡œ í¬í•¨)

        Args:
            filename (str): íŒŒì¼ëª…
            year (str): ë…„ë„ (4ìë¦¬)
            month (str): ì›” (2ìë¦¬)

        Returns:
            str: S3 í‚¤ ê²½ë¡œ
        """
        # íŒŒí‹°ì…˜ ê²½ë¡œ ìƒì„±: year=YYYY/mm=MM/filename
        partition_path = f"year={year}/mm={month}"

        if self.s3_prefix:
            s3_key = f"{self.s3_prefix}/{partition_path}/{filename}"
        else:
            s3_key = f"{partition_path}/{filename}"

        return s3_key

    def prepare_parquet_for_upload(self, parquet_file_path: str) -> Optional[str]:
        """
        =========================================================================
        ğŸ—‚ï¸ ì¤‘ìš”: Parquet íŒŒì¼ì—ì„œ íŒŒí‹°ì…˜ ì»¬ëŸ¼ ì œê±° ğŸ—‚ï¸
        =========================================================================

        ëª©ì : S3 íŒŒí‹°ì…˜ êµ¬ì¡°ë¡œ ì €ì¥ ì‹œ yyyy, month ì»¬ëŸ¼ì€ ë¶ˆí•„ìš”

        ì œê±° ëŒ€ìƒ:
        - yyyy ì»¬ëŸ¼: íŒŒí‹°ì…˜ ê²½ë¡œì˜ year=YYYYë¡œ ëŒ€ì²´
        - month ì»¬ëŸ¼: íŒŒí‹°ì…˜ ê²½ë¡œì˜ mm=MMìœ¼ë¡œ ëŒ€ì²´

        ìœ ì§€ ëŒ€ìƒ:
        - ë‚˜ë¨¸ì§€ ëª¨ë“  ì»¬ëŸ¼ (order_no, corp_code, corp_name, report_type ë“±)

        ìˆ˜ì •ë°©ë²•:
        - ë‹¤ì‹œ yyyy, monthë¥¼ í¬í•¨í•˜ë ¤ë©´ drop_columns ë¦¬ìŠ¤íŠ¸ì—ì„œ ì œê±°
        =========================================================================

        Args:
            parquet_file_path (str): ì›ë³¸ Parquet íŒŒì¼ ê²½ë¡œ

        Returns:
            str: ìˆ˜ì •ëœ Parquet íŒŒì¼ ê²½ë¡œ (ì„ì‹œ íŒŒì¼) ë˜ëŠ” None
        """
        try:
            # Parquet íŒŒì¼ ì½ê¸°
            df = pd.read_parquet(parquet_file_path)

            # íŒŒí‹°ì…˜ ì»¬ëŸ¼ ì œê±° (yyyy, month)
            drop_columns = ['yyyy', 'month']
            columns_to_drop = [col for col in drop_columns if col in df.columns]

            if columns_to_drop:
                df_cleaned = df.drop(columns=columns_to_drop)
                print(f"íŒŒí‹°ì…˜ ì»¬ëŸ¼ ì œê±°: {columns_to_drop}")
                print(f"  - ì œê±° ì „: {len(df.columns)}ê°œ ì»¬ëŸ¼")
                print(f"  - ì œê±° í›„: {len(df_cleaned.columns)}ê°œ ì»¬ëŸ¼")
            else:
                df_cleaned = df
                print("ì œê±°í•  íŒŒí‹°ì…˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            temp_file_path = parquet_file_path.replace('.parquet', '_temp_for_s3.parquet')
            df_cleaned.to_parquet(temp_file_path, index=False)

            return temp_file_path

        except Exception as e:
            print(f"Parquet íŒŒì¼ ì „ì²˜ë¦¬ ì˜¤ë¥˜ ({parquet_file_path}): {e}")
            return None

    def upload_file_to_s3(self, local_file_path: str, s3_key: str) -> bool:
        """
        íŒŒì¼ì„ S3ì— ì—…ë¡œë“œ

        Args:
            local_file_path (str): ë¡œì»¬ íŒŒì¼ ê²½ë¡œ
            s3_key (str): S3 í‚¤ ê²½ë¡œ

        Returns:
            bool: ì—…ë¡œë“œ ì„±ê³µ ì—¬ë¶€
        """
        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size = os.path.getsize(local_file_path)

        if self.dry_run:
            print(f"[DRY-RUN] S3 ì—…ë¡œë“œ ì‹œë®¬ë ˆì´ì…˜: s3://{self.bucket_name}/{s3_key}")
            print(f"  - íŒŒì¼ í¬ê¸°: {file_size:,} bytes")

            # í†µê³„ ì—…ë°ì´íŠ¸ (ì‹œë®¬ë ˆì´ì…˜)
            self.stats["files_uploaded"] += 1
            self.stats["total_size"] += file_size
            return True

        if not self.s3_client:
            print("S3 í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False

        try:
            # S3 ì—…ë¡œë“œ
            self.s3_client.upload_file(
                local_file_path,
                self.bucket_name,
                s3_key
            )

            print(f"S3 ì—…ë¡œë“œ ì„±ê³µ: s3://{self.bucket_name}/{s3_key}")
            print(f"  - íŒŒì¼ í¬ê¸°: {file_size:,} bytes")

            # í†µê³„ ì—…ë°ì´íŠ¸
            self.stats["files_uploaded"] += 1
            self.stats["total_size"] += file_size

            return True

        except Exception as e:
            error_msg = f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨ ({s3_key}): {e}"
            print(error_msg)
            self.stats["errors"].append(error_msg)
            self.stats["files_failed"] += 1
            return False

    def upload_parquet_files(self, parquet_files: List[str]) -> Dict:
        """
        ì—¬ëŸ¬ Parquet íŒŒì¼ì„ S3ì— íŒŒí‹°ì…”ë‹í•˜ì—¬ ì—…ë¡œë“œ

        Args:
            parquet_files (list): Parquet íŒŒì¼ ê²½ë¡œ ëª©ë¡

        Returns:
            dict: ì—…ë¡œë“œ ê²°ê³¼ í†µê³„
        """
        print(f"\n=== S3 íŒŒí‹°ì…”ë‹ ì—…ë¡œë“œ ì‹œì‘ ===")
        print(f"ì—…ë¡œë“œí•  íŒŒì¼ ìˆ˜: {len(parquet_files)}")
        if self.dry_run:
            print(f"[DRY-RUN MODE] ì‹¤ì œ ì—…ë¡œë“œ ì—†ì´ ì‹œë®¬ë ˆì´ì…˜ë§Œ ìˆ˜í–‰")

        if not self.dry_run and not self.s3_client:
            print("S3 í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return self.stats

        uploaded_files = []
        temp_files_to_cleanup = []

        for i, parquet_file in enumerate(parquet_files, 1):
            print(f"\n[{i}/{len(parquet_files)}] ì²˜ë¦¬ ì¤‘: {Path(parquet_file).name}")

            # 1. íŒŒí‹°ì…˜ ì •ë³´ ì¶”ì¶œ
            filename = Path(parquet_file).name
            partition_info = self.extract_partition_info(filename)

            if not partition_info:
                print(f"íŒŒí‹°ì…˜ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨, ê±´ë„ˆëœ€: {filename}")
                continue

            year = partition_info["year"]
            month = partition_info["month"]

            print(f"  íŒŒí‹°ì…˜: year={year}/mm={month}")

            # 2. Parquet íŒŒì¼ ì „ì²˜ë¦¬ (íŒŒí‹°ì…˜ ì»¬ëŸ¼ ì œê±°)
            temp_parquet_path = self.prepare_parquet_for_upload(parquet_file)
            if not temp_parquet_path:
                print(f"Parquet ì „ì²˜ë¦¬ ì‹¤íŒ¨, ê±´ë„ˆëœ€: {filename}")
                continue

            temp_files_to_cleanup.append(temp_parquet_path)

            # 3. S3 í‚¤ ìƒì„±
            s3_key = self.generate_s3_key(filename, year, month)
            print(f"  S3 ê²½ë¡œ: s3://{self.bucket_name}/{s3_key}")

            # 4. S3 ì—…ë¡œë“œ
            if self.upload_file_to_s3(temp_parquet_path, s3_key):
                uploaded_files.append({
                    "local_file": parquet_file,
                    "s3_key": s3_key,
                    "partition": f"year={year}/mm={month}"
                })

        # 5. ì„ì‹œ íŒŒì¼ ì •ë¦¬
        self.cleanup_temp_files(temp_files_to_cleanup)

        # 6. ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±
        self.generate_upload_report(uploaded_files)

        return self.stats

    def cleanup_temp_files(self, temp_files: List[str]):
        """ì„ì‹œ íŒŒì¼ ì •ë¦¬"""
        for temp_file in temp_files:
            try:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
                    print(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ: {Path(temp_file).name}")
            except Exception as e:
                print(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ ({temp_file}): {e}")

    def generate_upload_report(self, uploaded_files: List[Dict]):
        """ì—…ë¡œë“œ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±"""
        print(f"\n=== S3 ì—…ë¡œë“œ ê²°ê³¼ ë³´ê³ ì„œ ===")
        print(f"ì—…ë¡œë“œ ì„±ê³µ: {self.stats['files_uploaded']}ê°œ")
        print(f"ì—…ë¡œë“œ ì‹¤íŒ¨: {self.stats['files_failed']}ê°œ")
        print(f"ì´ ì—…ë¡œë“œ í¬ê¸°: {self.stats['total_size']:,} bytes")

        if uploaded_files:
            print(f"\nì—…ë¡œë“œëœ íŒŒì¼ ëª©ë¡:")
            for file_info in uploaded_files:
                print(f"  - {Path(file_info['local_file']).name}")
                print(f"    â””â”€ s3://{self.bucket_name}/{file_info['s3_key']}")

        if self.stats["errors"]:
            print(f"\në°œìƒí•œ ì˜¤ë¥˜ ({len(self.stats['errors'])}ê°œ):")
            for error in self.stats["errors"][:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                print(f"  - {error}")
            if len(self.stats["errors"]) > 5:
                print(f"  ... ì´ {len(self.stats['errors'])}ê°œ ì˜¤ë¥˜")

    def test_s3_connection(self) -> bool:
        """S3 ì—°ê²° í…ŒìŠ¤íŠ¸"""
        if not self.s3_client:
            return False

        try:
            # ë²„í‚· ì¡´ì¬ í™•ì¸
            self.s3_client.head_bucket(Bucket=self.bucket_name)
            print(f"S3 ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ: {self.bucket_name}")
            return True
        except Exception as e:
            print(f"S3 ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return False


def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    uploader = S3Uploader()

    # S3 ì—°ê²° í…ŒìŠ¤íŠ¸
    if uploader.test_s3_connection():
        print("S3 ì—°ê²°ì´ ì •ìƒì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("S3 ì—°ê²° ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    main()